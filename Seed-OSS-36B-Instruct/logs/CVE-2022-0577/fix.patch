--- a/scrapy/downloadermiddlewares/redirect.py
+++ b/scrapy/downloadermiddlewares/redirect.py
@@ -1,4 +1,5 @@
 import logging
+import fnmatch
 from urllib.parse import urljoin, urlparse
 
 from w3lib.url import safe_url_string
@@ -29,6 +30,18 @@
     def _redirect(self, redirected, request, spider, reason):
         ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)
         redirects = request.meta.get('redirect_times', 0) + 1
+
+        # Validate redirect URL against spider's allowed domains
+        parsed_url = urlparse(redirected.url)
+        netloc = parsed_url.netloc
+        if hasattr(spider, 'allowed_domains') and spider.allowed_domains:
+            if not any(fnmatch.fnmatch(netloc, domain) for domain in spider.allowed_domains):
+                logger.debug(
+                    "Redirect to %(url)s blocked: Not in allowed domains %(domains)s",
+                    {"url": redirected.url, "domains": spider.allowed_domains},
+                    extra={"spider": spider}
+                )
+                raise IgnoreRequest("Redirect URL not in allowed domains")
 
         if ttl and redirects <= self.max_redirect_times:
             redirected.meta['redirect_times'] = redirects
@@ -107,7 +120,8 @@
         interval, url = get_meta_refresh(response,
                                          ignore_tags=self._ignore_tags)
         if url and interval < self._maxdelay:
-            redirected = self._redirect_request_using_get(request, url)
+            redirected_url = urljoin(request.url, url)
+            redirected = self._redirect_request_using_get(request, redirected_url)
             return self._redirect(redirected, request, spider, 'meta refresh')
 
         return response

